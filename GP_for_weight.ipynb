{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processus Gaussien pour prédiction poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_save = os.getcwd() + \"/data/dataset.csv\"\n",
    "\n",
    "with open(file_save, 'r') as f:\n",
    "    df = pd.read_csv(file_save, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up\n",
    "df1 = df.copy(deep=True)\n",
    "columns_to_drop = ['Masse_Osseuse', 'Masse_Musculaire', 'Masse_Hydrique', 'MG%', 'BMR', 'Lipides', 'Glucides', 'Proteines', 'exo_duree', 'exo_cals_bruts', 'Depense_cal_totale', 'cal_deficit']\n",
    "df1.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visu\n",
    "\n",
    "fig, axs = plt.subplots(2,2,figsize=(24,8))\n",
    "\n",
    "for i,name in enumerate(df1.columns):\n",
    "    r = i%2\n",
    "    c = i//2\n",
    "    axs[r,c].set_title(name)\n",
    "    df1[name].plot(ax=axs[r,c])\n",
    "    axs[r,c].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to check that datapoints follow by one day\n",
    "# and deal with holes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : moyennage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_average = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_moyenne(df, length_average=length_average):\n",
    "    \"\"\"prend la dataframe et moyenne les valeurs suivant length_average\"\"\"\n",
    "    df = df.rolling(window=length_average).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get_df_moyenne(df1)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : create targets, which are next days's Masse_Totale and Masse_Grasse values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(df=df2):\n",
    "    \"\"\"créé les targets\n",
    "    \"\"\"\n",
    "    df2 = pd.concat([df.shift(1), df['Masse_Totale'], df[]'Masse_Grasse']], axis=1)\n",
    "    df2.columns = ['MT', 'MG', 'Cals', 'Exos', 'MT+1', 'MG+1']\n",
    "    df2.dropna(inplace=True)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = create_targets(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : date de démarrage des données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "month = 5\n",
    "day = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def crop_dataset(df=df2, year=year, month=month, day=day):\n",
    "    \"\"\"supprime les données avant year,month, date\n",
    "    \"\"\"\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df[df.index >= datetime(year=year, month=month, day=day)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = crop_dataset().copy(deep=True)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Construit les train et test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longueur du test set\n",
    "LAST = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(df=df2, last=LAST):\n",
    "    \"\"\"create train and test datasets\"\"\"\n",
    "    X = df2[['MT', 'MG', 'Cals', 'Exos']]\n",
    "    y_mt = df2['MT+1']\n",
    "    y_mg = df2['MG+1']\n",
    "    \n",
    "    X_train = X[:-LAST]\n",
    "    X_test = X[-LAST:]\n",
    "\n",
    "    y_mt_train = y_mt[:-LAST]\n",
    "    y_mt_test = y_mt[-LAST:]\n",
    "\n",
    "    y_mg_train = y_mg[:-LAST]\n",
    "    y_mg_test = y_mg[-LAST:]\n",
    "    \n",
    "    return X_train, X_test, y_mt_train, y_mt_test, y_mg_train, y_mg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_mt_train, y_mt_test, y_mg_train, y_mg_test = create_train_test_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancie un GP par output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process\n",
    "\n",
    "def get_gpr(length_scale=7, length_scale_bounds=(1e-1, 1e4), noise_level=2.0, noise_level_bounds=(1e-6, 1e2)):\n",
    "    \"\"\"Instancie un GPR avec un kernel Gaussien + kernel bruit blanc\"\"\"\n",
    "\n",
    "    kernel = RBF(\n",
    "        length_scale=length_scale,\n",
    "        length_scale_bounds=length_scale_bounds\n",
    "        ) \\\n",
    "        + WhiteKernel(\n",
    "            noise_level=noise_level,\n",
    "            noise_level_bounds=noise_level_bounds\n",
    "            )\n",
    "    gpr = GaussianProcessRegressor(kernel = kernel, alpha=0.0, random_state=42, normalize_y=True, n_restarts_optimizer=9 )\n",
    "    \n",
    "    return gpr, kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_mt, kernel_mt = get_gpr()\n",
    "gpr_mg, kernel_mg = get_gpr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_mt.fit(X_train, y_mt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_mg.fit(X_train, y_mg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction_mt, std_prediction_mt = gpr_mt.predict(X_train, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction_mg, std_prediction_mg = gpr_mg.predict(X_train, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(X_train, y_train, y_mean, y_std, gpr, kernel, titre):\n",
    "    \"\"\"utility function pour afficher perf sur training set\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    abscisses = np.arange(X_train.shape[0])\n",
    "    ax.scatter(abscisses, y_train, marker='o', label=\"Observations\", color='blue')\n",
    "    ax.plot(abscisses, y_mean, marker='.', label=\"Mean prediction\", color='green')\n",
    "    ax.set_title(\n",
    "        titre +\\\n",
    "        f\" {X_train.shape[0]} points\\n\" +\\\n",
    "        f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \" +\\\n",
    "        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\\n\" +\\\n",
    "        f\"données moyennées sur {length_average} jour(s)\"\n",
    "    )\n",
    "    ax.grid(True)\n",
    "    ax.fill_between(\n",
    "        abscisses,\n",
    "        y_mean - 1.96 * y_std,\n",
    "        y_mean + 1.96 * y_std,\n",
    "        alpha=0.5,\n",
    "        label=r\"95% confidence interval\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_training(X_train, y_mt_train, mean_prediction_mt, std_prediction_mt, gpr_mt, kernel_mt, \"Train set MT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_training(X_train, y_mg_train, mean_prediction_mg, std_prediction_mg, gpr_mg, kernel_mg, \"Train set MG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(f\"Train set : MAE MT = {mean_absolute_error(y_mt_train, mean_prediction_mt)}\")\n",
    "print(f\"Train set : MSE MT = {mean_squared_error(y_mt_train, mean_prediction_mt)}\")\n",
    "print(f\"Train set : MAE MG = {mean_absolute_error(y_mg_train, mean_prediction_mg)}\")\n",
    "print(f\"Train set : MSE MG = {mean_squared_error(y_mg_train, mean_prediction_mg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inférences J+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(X_test, y_test, y_mean, y_std, gpr, kernel, titre):\n",
    "    \"\"\"utility function pour afficher perf sur test set avec CI\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    abscisses = np.arange(X_test.shape[0])\n",
    "    ax.scatter(abscisses, y_test, marker='o', label=\"Observations\", color='blue')\n",
    "    ax.plot(abscisses, y_mean, marker='.', label=\"Mean prediction\", color='green')\n",
    "    ax.set_title(\n",
    "        titre +\\\n",
    "        f\" {X_test.shape[0]} points\\n\" +\\\n",
    "        f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \" +\\\n",
    "        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\\n\" +\\\n",
    "        f\"données moyennées sur {length_average} jour(s)\"\n",
    "    )\n",
    "    ax.grid(True)\n",
    "    ax.fill_between(\n",
    "        abscisses,\n",
    "        y_mean - 1.96 * y_std,\n",
    "        y_mean + 1.96 * y_std,\n",
    "        alpha=0.5,\n",
    "        label=r\"95% confidence interval\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction_mt, std_prediction_mt = gpr_mt.predict(X_test, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_inference(X_test, y_mt_test, mean_prediction_mt, std_prediction_mt, gpr_mt, kernel_mt, \"Test set MT J+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction_mg, std_prediction_mg = gpr_mg.predict(X_test, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_inference(X_test, y_mg_test, mean_prediction_mg, std_prediction_mg, gpr_mg, kernel_mg, \"Test set MG J+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set J+1 : MAE MT = {mean_absolute_error(y_mt_test, mean_prediction_mt)}\")\n",
    "print(f\"Test set J+1 : MSE MT = {mean_squared_error(y_mt_test, mean_prediction_mt)}\")\n",
    "print(f\"Test set J+1 : MAE MG = {mean_absolute_error(y_mg_test, mean_prediction_mg)}\")\n",
    "print(f\"Test set J+1 : MSE MG = {mean_squared_error(y_mg_test, mean_prediction_mg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inférence J+1 à J+N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizon de prédiction\n",
    "horizon = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = X_test[:horizon].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mt = []  # prédiction MT\n",
    "y_pred_mg = []  # prédiction MG\n",
    "y_pred_mt_std = []  # écart-type prédiction MT\n",
    "y_pred_mg_std = []  # écart-type prédiction MG\n",
    "\n",
    "# initialisation des valeurs de départ\n",
    "next_mt = X_test_pred.iloc[0,0]\n",
    "next_mg = X_test_pred.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(len(X_test_pred)):\n",
    "    # on extrait les inputs du jour et on remplace les valeurs MT, MG du jour par celles prédites\n",
    "    row = X_test_pred.iloc[r:r+1,:].copy(deep=True)\n",
    "    mini_X_test = pd.DataFrame(\n",
    "        data = { 'MT' : next_mt,\n",
    "                'MG' : next_mg,\n",
    "                'Cals' : row['Cals'],\n",
    "                'Exos' : row['Exos']}\n",
    "    )\n",
    "    # on prédit MT à J+1\n",
    "    pred_mt, pred_mt_std = gpr_mt.predict(mini_X_test, return_std=True)\n",
    "    next_mt = pred_mt[0]\n",
    "    y_pred_mt.append(next_mt)\n",
    "    y_pred_mt_std.append(pred_mt_std[0])\n",
    "    # on prédit MG à J+1\n",
    "    pred_mg, pred_mg_std = gpr_mg.predict(mini_X_test, return_std=True)\n",
    "    next_mg = pred_mg[0]\n",
    "    y_pred_mg.append(next_mg)\n",
    "    y_pred_mg_std.append(pred_mg_std[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mt_std = np.array(y_pred_mt_std)\n",
    "y_pred_mg_std = np.array(y_pred_mg_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference_horizon(X_test, y_test, y_mean, y_std, gpr, kernel, titre):\n",
    "    \"\"\"utility function pour afficher perf sur test set avec CI\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    abscisses = np.arange(X_test.shape[0])\n",
    "    ax.scatter(abscisses, y_test[:horizon], marker='o', label=\"Observations\", color='blue')\n",
    "    ax.plot(abscisses, y_mean, marker='.', label=\"Mean prediction\", color='green')\n",
    "    ax.set_title(f\"Test set {X_test.shape[0]} points. Inférence J+2 avec valeur prédite à J+1\\n\" f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \"\n",
    "        f\"{gpr_mt.log_marginal_likelihood(gpr.kernel_.theta)}\\n\" \\\n",
    "            f\"données moyennées sur {length_average} jours\" \n",
    "    )\n",
    "    ax.grid(True)\n",
    "    ax.fill_between(\n",
    "        abscisses,\n",
    "        y_mean - 1.96 * y_std,\n",
    "        y_mean + 1.96 * y_std,\n",
    "        alpha=0.5,\n",
    "        label=r\"95% confidence interval\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_inference_horizon(X_test_pred, y_mt_test, y_pred_mt, y_pred_mt_std, gpr_mt, kernel_mt, \"Prediction longue MT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_inference_horizon(X_test_pred, y_mg_test, y_pred_mg, y_pred_mg_std, gpr_mg, kernel_mg, \"Prediction longue MG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set J+{horizon} : MAE MT = {mean_absolute_error(y_mt_test[:horizon], y_pred_mt)}\")\n",
    "print(f\"Test set J+{horizon} : MSE MT = {mean_squared_error(y_mt_test[:horizon], y_pred_mt)}\")\n",
    "print(f\"Test set J+{horizon} : MAE MG = {mean_absolute_error(y_mg_test[:horizon], y_pred_mg)}\")\n",
    "print(f\"Test set J+{horizon} : MSE MG = {mean_squared_error(y_mg_test[:horizon], y_pred_mg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamètres\n",
    "\n",
    "length_average_list = [3] # durée moyennage des données\n",
    "start_date_list = [ (2020, 9, 1)]  # démarrage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for length_average, start_date in itertools.product(length_average_list, start_date_list):\n",
    "    \n",
    "    year, month, day = start_date\n",
    "    \n",
    "    # create dataset\n",
    "    df2 = get_df_moyenne(df1, length_average=length_average)  # moyenne les données\n",
    "    df2 = create_targets(df2) # retire les données non-utilisées\n",
    "    df2 = crop_dataset(year=year, month=month, day=day).copy(deep=True)\n",
    "    X_train, X_test, y_mt_train, y_mt_test, y_mg_train, y_mg_test = create_train_test_sets()\n",
    "    \n",
    "    # create and train models\n",
    "    gpr_mt, kernel_mt = get_gpr()\n",
    "    gpr_mt.fit(X_train, y_mt_train)\n",
    "    gpr_mg, kernel_mg = get_gpr()  \n",
    "    gpr_mg.fit(X_train, y_mg_train)\n",
    "    print(f\"Log marginal likelihood MT : {gpr_mt.log_marginal_likelihood(gpr_mt.kernel_.theta)}\")\n",
    "    print(f\"Log marginal likelihood MG : {gpr_mg.log_marginal_likelihood(gpr_mg.kernel_.theta)}\")\n",
    "    \n",
    "    # performance metrics sur training set\n",
    "    mean_prediction_mt, std_prediction_mt = gpr_mt.predict(X_train, return_std=True)\n",
    "    mean_prediction_mg, std_prediction_mg = gpr_mg.predict(X_train, return_std=True) \n",
    "    print(f\"Train set : MAE MT = {mean_absolute_error(y_mt_train, mean_prediction_mt)}\")\n",
    "    print(f\"Train set : MSE MT = {mean_squared_error(y_mt_train, mean_prediction_mt)}\")\n",
    "    print(f\"Train set : MAE MG = {mean_absolute_error(y_mg_train, mean_prediction_mg)}\")\n",
    "    print(f\"Train set : MSE MG = {mean_squared_error(y_mg_train, mean_prediction_mg)}\")\n",
    "    \n",
    "    # inférence J+1, performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
